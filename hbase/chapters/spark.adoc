[[spark]]
== HBase 和 Spark ==

link:http://spark.apache.org/[Apache Spark] 是一个分布式内存数据处理框架，在大多数情况下可以替代MapReduce。

有关 Spark 自身的详细情况请参考 Spark 的官方网站，这里不做过多描述。
这个章节，我们重点讲述和HBase有关的4个方面的内容，分别是：

Basic Spark::
  连接HBase，形成 Spark DAG.
Spark Streaming::
  在Spark Streaming 里连接HBase
Spark Bulk Load::
  直接写HBase HFile 文件，用于HBase 桶插入场景。
SparkSQL/DataFrames::
  写SparkSQL来和 HBase 表进行交互。

以下就这四个方面做详细介绍。

=== Basic Spark ===

该章节描述 Spark 如何在最底层以及最简单方式和 HBase 进行集成。其他章节的内容就是以此为基础。

Spark 和 HBase 集成的根本是  HBaseContext.  HBaseContext
获取  HBase 配置，如何把配置推入到 Spark 执行器中。 这使得我们可以在每个 Spark 执行器中以固定路径方式获得一个 HBase 连接。

Spark 执行器既可以和 Region 服务在一个节点上，也可以分离。考虑到每个 Spark 执行器均为多线程客户端程序。下面允许你运行在执行器上的任意 Spark 任务都可以访问共享的连接对象。

.HBaseContext 用例
====

下面的例子演示了如果用Scala语言操作 HBaseContext 在 RDD 上执行 `foreachPartition` ：

[source, scala]
----
val sc = new SparkContext("local", "test")
val config = new HBaseConfiguration()

...

val hbaseContext = new HBaseContext(sc, config)

rdd.hbaseForeachPartition(hbaseContext, (it, conn) => {
 val bufferedMutator = conn.getBufferedMutator(TableName.valueOf("t1"))
 it.foreach((putRecord) => {
. val put = new Put(putRecord._1)
. putRecord._2.foreach((putValue) => put.addColumn(putValue._1, putValue._2, putValue._3))
. bufferedMutator.mutate(put)
 })
 bufferedMutator.flush()
 bufferedMutator.close()
})
----

以下是 Java 语言的实现：

[source, java]
----
JavaSparkContext jsc = new JavaSparkContext(sparkConf);

try {
  List<byte[]> list = new ArrayList<>();
  list.add(Bytes.toBytes("1"));
  ...
  list.add(Bytes.toBytes("5"));

  JavaRDD<byte[]> rdd = jsc.parallelize(list);
  Configuration conf = HBaseConfiguration.create();

  JavaHBaseContext hbaseContext = new JavaHBaseContext(jsc, conf);

  hbaseContext.foreachPartition(rdd,
      new VoidFunction<Tuple2<Iterator<byte[]>, Connection>>() {
   public void call(Tuple2<Iterator<byte[]>, Connection> t)
        throws Exception {
    Table table = t._2().getTable(TableName.valueOf(tableName));
    BufferedMutator mutator = t._2().getBufferedMutator(TableName.valueOf(tableName));
    while (t._1().hasNext()) {
      byte[] b = t._1().next();
      Result r = table.get(new Get(b));
      if (r.getExists()) {
       mutator.mutate(new Put(b));
      }
    }

    mutator.flush();
    mutator.close();
    table.close();
   }
  });
} finally {
  jsc.stop();
}
----
====

所有 集成 Spark 和 HBase 的功能同时支持 Scala 和 Java，本篇的剩余章节重点给出 Scala 的例子。

上述例子展示了如何使用一个连接来执行 foreachPartition. 其他开箱即用的 Spark 基本功能如下：

// tag::spark_base_functions[]
`bulkPut`:: 大规模并发 HBase puts 操作。
`bulkDelete`:: 大规模并发 HBase delete 操作。
`bulkGet`:: 创建新的 RDD 用于大规模并发 HBase get 操作。
`mapPartition`:: 利用 HBase 连接对象做 Spark Map 功能。
`hBaseRDD`:: 创建 RDD 用于简化分布式 scan 操作。
// end::spark_base_functions[]

对于以上功能的详细描述和使用指南，可以参考 HBase-Spark 模块。

=== Spark Streaming ===
http://spark.apache.org/streaming/[Spark Streaming] 是一个在 Spark 之上的批量式流处理框架。
HBase 和 Spark Streaming 紧密结合，HBase 可以为 Spark Streaming 带来以下好处：

* 快速获取参考数据或配置数据。
* A place to store counts or aggregates in a way that supports Spark Streaming
promise of _only once processing_.

The HBase-Spark module’s integration points with Spark Streaming are similar to
its normal Spark integration points, in that the following commands are possible
straight off a Spark Streaming DStream.

include::spark.adoc[tags=spark_base_functions]

.`bulkPut` 使用 DStreams 例子
====

以下是利用 DStreams 的 bulkPut 的例子。它和 RDD bulk put 非常像。

[source, scala]
----
val sc = new SparkContext("local", "test")
val config = new HBaseConfiguration()

val hbaseContext = new HBaseContext(sc, config)
val ssc = new StreamingContext(sc, Milliseconds(200))

val rdd1 = ...
val rdd2 = ...

val queue = mutable.Queue[RDD[(Array[Byte], Array[(Array[Byte],
    Array[Byte], Array[Byte])])]]()

queue += rdd1
queue += rdd2

val dStream = ssc.queueStream(queue)

dStream.hbaseBulkPut(
  hbaseContext,
  TableName.valueOf(tableName),
  (putRecord) => {
   val put = new Put(putRecord._1)
   putRecord._2.foreach((putValue) => put.addColumn(putValue._1, putValue._2, putValue._3))
   put
  })
----

`hbaseBulkPut` 函数有三个参数.
. hbaseContext 带有配置广播信息，使得我们可以在执行器里连接到 HBase
. 数据要写入的表名
. 把 DStream 转为 HBase Put 对象的函数
====

=== 批量加载 ===

利用 Spark 实现对 HBase 的数据批量加载有两种方式。 基本的批量加载功能可以应付包含有百万列的记录，这些列可以不固定。

另外一种是针对窄记录的，一般一条记录的列数不超过10000，这种方案的优势是高吞吐以及 shuffle 操作时低负载。

两种实现方式多少和 MapReduce 的批量加载过程类似：一个分区进程依据 region 分割点来对 rowkey 进行分区，然后按顺序把这些行键发送给 reducer，因为已经排序，所以 reducer 可以直接把写成 HFile 文件。

在 Spark 的术语中，批量加载是在 `repartitionAndSortWithinPartitions` 功能后面接一个循环 `foreachPartition` 功能来实现的。

首先，我们看下基本的批量加载功能例子。

.批量加载例子
====

以下代码展示了使用 Spark 实现批量加载功能。

[source, scala]
----
val sc = new SparkContext("local", "test")
val config = new HBaseConfiguration()

val hbaseContext = new HBaseContext(sc, config)

val stagingFolder = ...
val rdd = sc.parallelize(Array(
      (Bytes.toBytes("1"),
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("a"), Bytes.toBytes("foo1"))),
      (Bytes.toBytes("3"),
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("b"), Bytes.toBytes("foo2.b"))), ...

rdd.hbaseBulkLoad(TableName.valueOf(tableName),
  t => {
   val rowKey = t._1
   val family:Array[Byte] = t._2(0)._1
   val qualifier = t._2(0)._2
   val value = t._2(0)._3

   val keyFamilyQualifier= new KeyFamilyQualifier(rowKey, family, qualifier)

   Seq((keyFamilyQualifier, value)).iterator
  },
  stagingFolder.getPath)

val load = new LoadIncrementalHFiles(config)
load.doBulkLoad(new Path(stagingFolder.getPath),
  conn.getAdmin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
----
====

`hbaseBulkLoad` 函数接受三个必要参数：

. 期望要加载数据的表名

. 实现 RDD 转 tuple 的函数，其中tuple 由 key-value 对组成。 Key 是一个 KeyFamilyQualifer 对象，value
是 单元格的值。 KeyFamilyQualifer 对象包括  RowKey, Column Family, 和 Column Qualifier.
shuffle 依据上述三个值进行排序。

. 保存 HFile 文件的临时目录。

接下来， Spark 的批量加载命令使用  HBase 的 LoadIncrementalHFiles 对象来加载最近创建的 HFile 文件到 HBase 表中。

.批量加载的额外参数

hbaseBulkLoad 函数中，我们还可以设置以下属性。

* HFile 的最大文件大小
* 从合并操作里排出 HFile 文件的标志
* 设置列族的 压缩、布隆类型、块大小以及数据库编码等

.使用额外参数
====

[source, scala]
----
val sc = new SparkContext("local", "test")
val config = new HBaseConfiguration()

val hbaseContext = new HBaseContext(sc, config)

val stagingFolder = ...
val rdd = sc.parallelize(Array(
      (Bytes.toBytes("1"),
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("a"), Bytes.toBytes("foo1"))),
      (Bytes.toBytes("3"),
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("b"), Bytes.toBytes("foo2.b"))), ...

val familyHBaseWriterOptions = new java.util.HashMap[Array[Byte], FamilyHFileWriteOptions]
val f1Options = new FamilyHFileWriteOptions("GZ", "ROW", 128, "PREFIX")

familyHBaseWriterOptions.put(Bytes.toBytes("columnFamily1"), f1Options)

rdd.hbaseBulkLoad(TableName.valueOf(tableName),
  t => {
   val rowKey = t._1
   val family:Array[Byte] = t._2(0)._1
   val qualifier = t._2(0)._2
   val value = t._2(0)._3

   val keyFamilyQualifier= new KeyFamilyQualifier(rowKey, family, qualifier)

   Seq((keyFamilyQualifier, value)).iterator
  },
  stagingFolder.getPath,
  familyHBaseWriterOptions,
  compactionExclude = false,
  HConstants.DEFAULT_MAX_FILE_SIZE)

val load = new LoadIncrementalHFiles(config)
load.doBulkLoad(new Path(stagingFolder.getPath),
  conn.getAdmin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
----
====

现在，我们看如何实现窄记录的批量加载。

.使用窄记录批量加载
====

[source, scala]
----
val sc = new SparkContext("local", "test")
val config = new HBaseConfiguration()

val hbaseContext = new HBaseContext(sc, config)

val stagingFolder = ...
val rdd = sc.parallelize(Array(
      ("1",
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("a"), Bytes.toBytes("foo1"))),
      ("3",
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("b"), Bytes.toBytes("foo2.b"))), ...

rdd.hbaseBulkLoadThinRows(hbaseContext,
      TableName.valueOf(tableName),
      t => {
        val rowKey = t._1

        val familyQualifiersValues = new FamiliesQualifiersValues
        t._2.foreach(f => {
          val family:Array[Byte] = f._1
          val qualifier = f._2
          val value:Array[Byte] = f._3

          familyQualifiersValues +=(family, qualifier, value)
        })
        (new ByteArrayWrapper(Bytes.toBytes(rowKey)), familyQualifiersValues)
      },
      stagingFolder.getPath,
      new java.util.HashMap[Array[Byte], FamilyHFileWriteOptions],
      compactionExclude = false,
      20)

val load = new LoadIncrementalHFiles(config)
load.doBulkLoad(new Path(stagingFolder.getPath),
  conn.getAdmin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
----
====

注意到这种批量加载与第一个的最大不同是其返回的 tuple 的元素，第一个值是行键，第二个是 FamiliesQualifiersValues 对象，它包含了该条记录所有数据。

=== SparkSQL/DataFrames ===

http://spark.apache.org/sql/[SparkSQL] 是一个 Spark 的子项目，它能够把 SQL 转为一个 Spark
 DAG。另外，SparkSQL 也是DataFrames 的重度使用者。 DataFrames 就像一个带模式(schema)信息的 RDD。

HBase-Spark 模块支持 Spark SQL 和 DataFrames, 他们允许直接在 HBase 表上写 SparkSQL。另外 HBase-Spark
把查询过滤逻辑下推到 HBase。

在 HBaseSparkConf 里，有四个和时间戳有关的参数可以设置，他们分别是 TIMESTAMP,
MIN_TIMESTAMP, MAX_TIMESTAMP 和 MAX_VERSIONS. 用户可以使用不同的时间戳或使用MIN_TIMESTAMP 和 MAX_TIMESTAMP 构成时间范围来查询记录。

.使用不同时间戳进行查询
====

下面展示了如何使用不同的时间戳来加载 DataFrame
tsSpecified 由用户指定.
HBaseTableCatalog 定义了  HBase 和 Relation 关系模型。
writeCatalog 定义了模式映射条目
----
val df = sqlContext.read
      .options(Map(HBaseTableCatalog.tableCatalog -> writeCatalog, HBaseSparkConf.TIMESTAMP -> tsSpecified.toString))
      .format("org.apache.hadoop.hbase.spark")
      .load()
----

下面展示了如何使用不同的时间戳来加载 DataFrame
oldMs 由用户指定
----
val df = sqlContext.read
      .options(Map(HBaseTableCatalog.tableCatalog -> writeCatalog, HBaseSparkConf.MIN_TIMESTAMP -> "0",
        HBaseSparkConf.MAX_TIMESTAMP -> oldMs.toString))
      .format("org.apache.hadoop.hbase.spark")
      .load()
----

加载 DataFrame 后，用户就可以查询了。
----
    df.registerTempTable("table")
    sqlContext.sql("select count(col1) from table").show
----
====

==== 谓词下推 ====

这里展示了 HBase-Spark 里有两种谓词下推的实现方式。
第一个例子是行键上的过滤逻辑的谓词下推。 HBase-Spark 会把在行键的上的过滤转变为一套 Get 或/和 Scan 命令。

NOTE: Scan 是分布式的 Scan，不是单个客户端的 Scan 操作。

如果一个查询看起来像下面这样，那么逻辑将会下推，变成使用三个 Get 操作和零个 Scan 操作来获得对应的记录。这里之所以用 Get 操作是因为所有的查询都是等值(`equal`)操作。

[source,sql]
----
SELECT
  KEY_FIELD,
  B_FIELD,
  A_FIELD
FROM hbaseTmp
WHERE (KEY_FIELD = 'get1' or KEY_FIELD = 'get2' or KEY_FIELD = 'get3')
----

现在，让我们看下用用两个 Scan 操作的谓词下推例子。

[source, sql]
----
SELECT
  KEY_FIELD,
  B_FIELD,
  A_FIELD
FROM hbaseTmp
WHERE KEY_FIELD < 'get2' or KEY_FIELD > 'get3'
----

在这个例子中，我们将会使用 0 个 Get 以及 2 个 Scan。一个 Scan 操作获得表的第一行到 'get2' 的记录，另外一个 Scan 操作则获得从 'get3' 一直到最后一条记录的所有记录。

下面这个查询是演示范围查询的例子。这个例子中的范围重叠了。代码将足够智能到把下面的两个范围查询变成单个范围查询。

[source, sql]
----
SELECT
  KEY_FIELD,
  B_FIELD,
  A_FIELD
FROM hbaseTmp
WHERE
  (KEY_FIELD >= 'get1' and KEY_FIELD <= 'get3') or
  (KEY_FIELD > 'get3' and KEY_FIELD <= 'get5')
----

第二个例子中的下推功能由 HBase-Spark 提供，它可以下推针对列和单元格值的过滤逻辑，就像针对行键的逻辑一样，所有的查询逻辑通过发送带有过滤对象的 Scan 操作来查询固化到最小数范围检查和等值检查

.SparkSQL 代码例子
====
该例子展示了如何使用 SQL 来和 HBase 交互。

[source, scala]
----
val sc = new SparkContext("local", "test")
val config = new HBaseConfiguration()

new HBaseContext(sc, TEST_UTIL.getConfiguration)
val sqlContext = new SQLContext(sc)

df = sqlContext.load("org.apache.hadoop.hbase.spark",
  Map("hbase.columns.mapping" ->
   "KEY_FIELD STRING :key, A_FIELD STRING c:a, B_FIELD STRING c:b",
   "hbase.table" -> "t1"))

df.registerTempTable("hbaseTmp")

val results = sqlContext.sql("SELECT KEY_FIELD, B_FIELD FROM hbaseTmp " +
  "WHERE " +
  "(KEY_FIELD = 'get1' and B_FIELD < '3') or " +
  "(KEY_FIELD >= 'get3' and B_FIELD = '8')").take(5)
----

上述代码可以分为三个部门来解释。

sqlContext.load 函数::
  在 sqlContext.load 函数里，我们看到有两个参数，第一个参数是指向 HBase DefaultSource 的 Spark 类，它充当 SparkSQL 和 HBase 的接口。

key-value 对映射::
  在本例中，映射里有两个 key， `hbase.columns.mapping` 和
  `hbase.table`.  `hbase.table` 告诉 SparkSQL 要用哪个 HBase 表。
  `hbase.columns.mapping` 给出 HBase 列转为 SparkSQL 列的逻辑。
+
 `hbase.columns.mapping` 是一个字符串，可以是以下格式
+
[source, scala]
----
(SparkSQL.ColumnName) (SparkSQL.ColumnType) (HBase.ColumnFamily):(HBase.Qualifier)
----
+
在这个例子中，我们定义了三个字段，因为 KEY_FIELD 没有列族，所以，它是行键。
+
----
KEY_FIELD STRING :key, A_FIELD STRING c:a, B_FIELD STRING c:b
----

registerTempTable 函数::
  这是 SparkSQL 的函数，它使得我们可以脱离 Scala 语言，而直接使用 SQL 访问 "hbaseTmp" 的方式来访问 HBase 表。

最后一个重点点是 `sqlContext.sql` 函数，它会把 SQL 下推到 DefaultSource 代码。这个命令返回结果是一个 DataFrame，其模式包含 KEY_FIELD 和 B_FIELD.
====
