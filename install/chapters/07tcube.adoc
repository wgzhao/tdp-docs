[[tcube]]
== TCube 安装与配置 ==

TCube 是 {prodshortname} 包含的数据操作平台，方便数据开发人员以及数据运维人员使用基于 WEB 的方式来管理和操作保存在 HDFS 上的数据。
它同时提供了基于 WEB 方式的 Hive 和 HBase 管理和查询方式，大大降低了数据使用人员的学习门槛。

=== 安装 ===

TCube 的安装非常简单。从介质中拷贝 tcube 包到需要安装的服务器上。用 root 账号登陆该服务器，然后执行下面的命令：

[source,shell]
....
./tcube-3.10-lnx64-rhel7.sh
....

正常安装下，应该可以类似如下的内容：
.....
# /tmp/tcube_3.10-lnx64.bin
Verifying archive integrity...  100%   All good.
Uncompressing TCube  100%
Begin installing TCube 3.10
Extracting package.
Setting up TCube.
TCube has installed into /usr/local directory
your can use service tcube start to run tcube
Once you start tcube,you can visit via http://localhost:8888
.....

=== 配置 ===

TCube 安装完毕后，配置文件默认位于 _/etc/tcube/conf_ 目录，主要的配置文件 `hue.ini`。配置文件采取 ini 格式，下面把主要的配置参数做说明。

==== desktop 配置 ====

|=====
| 参数     | 默认值     | 说明

| secret_key |   | 在会话的散列算法中使用

| http_host | 0.0.0.0 | 要绑定的 HTTP 主机

| http_port | 8888 | 要绑定的 HTTP 端口

| database | sqlite3 | 用于指定桌面数据库的配置选项

|=====

==== hadoop 配置 ====

hdfs_cluster::

|====

| 参数     | 默认值     | 说明

| fs_defaults | hdfs://localhost:8020 | 访问 HDFS 的方式，和 `fs.default.name` 等价

| webhdfs_url | http://locahost:14000/webhdfs/v1  | 访问 WebHDFS/HttpFS 服务的地址。

| hadoop_conf_dir | /etc/hadoop/conf | Hadoop 配置文件目录，即 `hdfs-site.xml` 所在的目录

|====

yarn_clusters::

|====
| 参数     | 默认值     | 说明
| proxy_api_url |  | ProxyServer API 地址
| resourcemanager_host | localhost | ResourceManger主机名或 IP 地址
| resourcemanager_api_url | http://localhost:8088 | ResourceManger API 的 URL
| spark_history_server_url | http://localhost:18080 | Spark History Server 的 URL
| resourcemanager_port | 8032 | ResourceManager 的服务端口
| is_yarn | True | 是否仅设置为 YARN 集群而没有 MR1 集群
|====

==== beeswax 配置 ====
|====
| 参数     | 默认值     | 说明
| hive_conf_dir | /etc/hive/conf | Hive 配置目录，即 `hive-site.xml` 所在的目录
| download_cell_limit | 10000 | 一个查询中能下载的记录数最大值
| hive_server_port | 10000 | 配置运行 HiveServer2 服务的端口
| hive_server_host | localhost | 运行 HiveServer2 的主机
|====

==== HBase 配置 ====

|====
| 参数     | 默认值     | 说明
| hbase_clusters | (Cluster\|localhost:9090) | 逗号分隔的 HBase Thrift 服务器，格式为 `(name\|host:port)`
| hbase_conf_dir | /etc/hbase/conf | HBase 配置目录，即 `hbase-site.xml` 文件所在目录
|====

==== Spark 配置 ====
|====
| 参数     | 默认值     | 说明
| livy_server_port | 8998 | Livy Server 运行端口
| sql_server_port | 10000 | SparkSQL 服务运行的端口
| sql_server_host | localhost | 运行 SparkSQL 的服务器名称或 IP 地址
| livy_server_host | localhost | 运行 Livy Server 服务的服务器名称或者 IP 地址
| livy_server_session_kind | process | livy 的运行模式，目前仅支持 `process` 和 `yarn` 两种
|====

==== librdbms 配置 ====

|====
| 参数     | 默认值     | 说明
| engine | django.db.backends.sqlite3 | 数据库引擎，可以为 postgresql_psycopg2,mysql,sqlite3
| name |  | 数据库名称或数据库文件路径。如果已提供，则不允许选择其他数据库
| host |  | 数据库主机
| port | 0 | 数据库端口
| user | | 数据库用户名
| password | | 数据库密码
| password_script | | 执行该脚本用来生成数据密码
| options | {}  | 连接时发送到服务器的数据库选项
|====
